#!/usr/bin/env python
# -*- coding: UTF-8 -*-

'''
@Project ï¼šnano_rag 
@File    ï¼šconvert_to_onnx.py
@Author  ï¼šfengzhengxiong
@Date    ï¼š2025/12/30 09:45 
'''

import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForSequenceClassification
from transformers import AutoTokenizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig
from optimum.onnxruntime import ORTQuantizer


def convert_reranker():
    # 1. è·¯å¾„è®¾ç½®
    project_root = Path(__file__).parent.parent
    input_model_path = project_root / "models/bge-reranker-base"
    output_model_path = project_root / "models/bge-reranker-base-onnx"

    print(f"ğŸ”„ æ­£åœ¨åŠ è½½ PyTorch æ¨¡å‹: {input_model_path}")

    if not input_model_path.exists():
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æºæ¨¡å‹ {input_model_path}")
        return

    # 2. å¯¼å‡º ONNX (Export)
    # è¿™ä¸€æ­¥ä¼šæŠŠ PyTorch æƒé‡å›¾è½¬æ¢ä¸º ONNX è®¡ç®—å›¾
    print("â³ æ­£åœ¨å¯¼å‡ºä¸º ONNX æ ¼å¼ (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    model = ORTModelForSequenceClassification.from_pretrained(
        input_model_path,
        export=True
    )
    tokenizer = AutoTokenizer.from_pretrained(input_model_path)

    # 3. é‡åŒ– (Quantization) -> INT8
    # é’ˆå¯¹ CPU (AVX512/AVX2) è¿›è¡ŒåŠ¨æ€é‡åŒ–
    print("ğŸ“‰ æ­£åœ¨è¿›è¡Œ INT8 é‡åŒ–...")
    quantizer = ORTQuantizer.from_pretrained(model)
    qconfig = AutoQuantizationConfig.avx512(is_static=False, per_channel=True)

    quantizer.quantize(
        save_dir=output_model_path,
        quantization_config=qconfig,
    )

    # ä¿å­˜ Tokenizer (æ¨ç†æ—¶è¿˜éœ€è¦å®ƒ)
    tokenizer.save_pretrained(output_model_path)

    print(f"âœ… è½¬æ¢å®Œæˆï¼é‡åŒ–æ¨¡å‹å·²ä¿å­˜è‡³: {output_model_path}")
    print("ğŸ‘‰ æ–‡ä»¶å: model_quantized.onnx (è¿™æ˜¯æˆ‘ä»¬è¦åŠ è½½çš„æ–‡ä»¶)")


if __name__ == "__main__":
    convert_reranker()