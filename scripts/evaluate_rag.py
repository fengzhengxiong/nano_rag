#!/usr/bin/env python
# -*- coding: UTF-8 -*-

'''
@Project ï¼šnano_rag 
@File    ï¼ševaluate_rag.py
@Author  ï¼šfengzhengxiong
@Date    ï¼š2025/12/29 16:57 
'''

import os
import sys
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    context_precision,
    context_recall,
    faithfulness,
    answer_relevancy,
)

from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings


sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.nano_rag.config.loader import get_resolved_config
from src.nano_rag.application import RAGApplication

# 1. é…ç½®è¯„å§”æ¨¡å‹ (Judge Model)
# æˆ‘ä»¬ä½¿ç”¨ DeepSeek V3 ä½œä¸ºè£åˆ¤ï¼Œå®ƒè¶³å¤Ÿèªæ˜ä¸”ä¾¿å®œ
config = get_resolved_config()
judge_llm = ChatOpenAI(
    base_url=config.llm.base_url,
    api_key=config.llm.api_key,
    model=config.llm.model_name,
    temperature=0
)

# 2. é…ç½® Embedding æ¨¡å‹ (ç”¨äºè¯„ä¼°ç›¸ä¼¼åº¦)
# å¤ç”¨æœ¬åœ°çš„ BGE æ¨¡å‹
embedding_model = HuggingFaceEmbeddings(
    model_name=config.embedding.model_name,
    encode_kwargs={'normalize_embeddings': True}
)


def prepare_test_data(app: RAGApplication):
    """
    å‡†å¤‡æµ‹è¯•æ•°æ®é›† (Golden Dataset)ã€‚
    """
    questions = [
        "FZX é›†å›¢ Q4 ä¼ä¸šçº§ RAG ä¸€ä½“æœºçš„è¥æ”¶æ˜¯å¤šå°‘ï¼Ÿ",
        "å“ªä¸ªäº§å“çº¿çš„æ¯›åˆ©ç‡æœ€é«˜ï¼Ÿ",
        "æ ¸å¿ƒæ¶æ„éƒ¨ä¸»è¦åœ¨å“ªé‡ŒåŠå…¬ï¼Ÿ",
        "æ•°æ®æ¸…æ´—æœåŠ¡çš„ç¯æ¯”å¢é•¿æ˜¯å¤šå°‘ï¼Ÿ",
    ]

    # ã€ä¿®å¤ç‚¹ã€‘è¿™é‡Œå¿…é¡»æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼Œä¸èƒ½æ˜¯åˆ—è¡¨çš„åˆ—è¡¨
    # æ—§å†™æ³•: [["180.0 ç™¾ä¸‡å…ƒ"], ...]  <-- æŠ¥é”™åŸå› 
    # æ–°å†™æ³•: ["180.0 ç™¾ä¸‡å…ƒ", ...]
    ground_truths = [
        "180.0 ç™¾ä¸‡å…ƒ",
        "AI å®‰å…¨ç½‘å…³ï¼Œæ¯›åˆ©ç‡ä¸º 72%",
        "åŒ—äº¬",
        "-5.0%",
    ]

    answers = []
    contexts = []

    print("ğŸš€ å¼€å§‹è¿è¡Œ RAG ç³»ç»Ÿç”Ÿæˆç­”æ¡ˆ...")

    import asyncio

    async def run_queries():
        for q in questions:
            print(f"Querying: {q} ...")
            resp = await app.query_service.ask(q, session_id="eval_bot")
            answers.append(resp.answer)
            # æå–å¬å›çš„ä¸Šä¸‹æ–‡å†…å®¹
            ctx_list = [doc.page_content for doc in resp.source_documents]
            contexts.append(ctx_list)

    asyncio.run(run_queries())

    # æ„å»º Ragas æ‰€éœ€çš„æ•°æ®é›†æ ¼å¼
    # Ragas v0.2+ ä¼šè‡ªåŠ¨å°† ground_truth æ˜ å°„ä¸º reference
    data = {
        "user_input": questions,  # æ–°ç‰ˆå»ºè®®ç”¨ user_input è€Œä¸æ˜¯ question
        "response": answers,  # æ–°ç‰ˆå»ºè®®ç”¨ response è€Œä¸æ˜¯ answer
        "retrieved_contexts": contexts,  # æ–°ç‰ˆå»ºè®®ç”¨ retrieved_contexts
        "reference": ground_truths  # æ–°ç‰ˆå»ºè®®ç”¨ reference
    }
    return Dataset.from_dict(data)


def main():
    print("ğŸ”„ åˆå§‹åŒ– RAG åº”ç”¨...")
    app = RAGApplication(config)

    print("ğŸ› ï¸ å‡†å¤‡æµ‹è¯•æ•°æ®...")
    dataset = prepare_test_data(app)

    print("âš–ï¸ å¼€å§‹ Ragas è¯„ä¼° (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    # è¿™ä¸€æ­¥ä¼šè°ƒç”¨ Judge LLM å¯¹æ¯ä¸€æ¡é—®ç­”è¿›è¡Œæ‰“åˆ†
    results = evaluate(
        dataset=dataset,
        metrics=[
            context_precision,  # æ£€ç´¢ç²¾åº¦ï¼šæ£€ç´¢åˆ°çš„å†…å®¹é‡Œæœ‰å¤šå°‘æ˜¯æœ‰ç”¨çš„ï¼Ÿ
            context_recall,  # æ£€ç´¢å¬å›ï¼šæ ‡å‡†ç­”æ¡ˆéœ€è¦çš„ä¿¡æ¯éƒ½æŸ¥åˆ°äº†å—ï¼Ÿ
            faithfulness,  # å¿ å®åº¦ï¼šç­”æ¡ˆæ˜¯å¦å®Œå…¨åŸºäºä¸Šä¸‹æ–‡ï¼ˆæ²¡å¹»è§‰ï¼‰ï¼Ÿ
            answer_relevancy,  # ç›¸å…³æ€§ï¼šç­”éæ‰€é—®äº†å—ï¼Ÿ
        ],
        llm=judge_llm,
        embeddings=embedding_model
    )

    print("\n" + "=" * 50)
    print("ğŸ“Š è¯„ä¼°æŠ¥å‘Š (Evaluation Report)")
    print("=" * 50)
    print(results)

    # å¯¼å‡ºä¸º Excel æ–¹ä¾¿ç»™è€æ¿çœ‹
    df = results.to_pandas()
    output_file = "evaluation_report.xlsx"
    df.to_excel(output_file, index=False)
    print(f"\nâœ… è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")


if __name__ == "__main__":
    main()